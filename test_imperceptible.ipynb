{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art import config\n",
    "from art.defences.preprocessor import Mp3Compression\n",
    "from art.utils import get_file\n",
    "\n",
    "from art.estimators.speech_recognition.pytorch_deep_speech import PyTorchDeepSpeech\n",
    "from art.attacks.evasion.imperceptible_asr.imperceptible_asr_pytorch import ImperceptibleASRPyTorch\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_waveform(waveform, title=\"\", sr=16000):\n",
    "  \"\"\"Display waveform plot and audio play UI.\"\"\"\n",
    "  plt.figure()\n",
    "  plt.title(title)\n",
    "  plt.plot(waveform)\n",
    "  ipd.display(ipd.Audio(waveform, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_amp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer = PyTorchDeepSpeech(\n",
    "  pretrained_model=\"librispeech\",\n",
    "  device_type=\"cpu\",\n",
    "  use_amp = use_amp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_attack = ImperceptibleASRPyTorch(\n",
    "    estimator=speech_recognizer,\n",
    "    eps=0.001,\n",
    "    max_iter_1=5,\n",
    "    max_iter_2=5,\n",
    "    learning_rate_1=0.00001,\n",
    "    learning_rate_2=0.001,\n",
    "    optimizer_1=torch.optim.Adam,\n",
    "    optimizer_2=torch.optim.Adam,\n",
    "    global_max_length=100000,\n",
    "    initial_rescale=1.0,\n",
    "    decrease_factor_eps=0.8,\n",
    "    num_iter_decrease_eps=5,\n",
    "    alpha=0.01,\n",
    "    increase_factor_alpha=1.2,\n",
    "    num_iter_increase_alpha=5,\n",
    "    decrease_factor_alpha=0.8,\n",
    "    num_iter_decrease_alpha=5,\n",
    "    win_length=2048,\n",
    "    hop_length=512,\n",
    "    n_fft=2048,\n",
    "    batch_size=2,\n",
    "    use_amp=use_amp,\n",
    "    opt_level=\"O1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound, sample_rate = torchaudio.load(\"/mnt/d/course_project/ECE720/data/dataset_timit/data/TEST/DR1/MDAB0/SA1.WAV.wav\")\n",
    "sound = sound.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, audio = wav.read(\"/mnt/d/course_project/ECE720/data/dataset_timit/data/TEST/DR1/MDAB0/SA1.WAV.wav\")\n",
    "y = np.array([\"She hate your dark suit in greasy wash water all year\".upper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_adv = asr_attack.generate(sound, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 45466)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save(\"imperceptible.wav\", src=torch.Tensor(sound_adv), sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_preprocessing = speech_recognizer.predict(sound_adv, batch_size=1, transcription_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SHE HAV YOUR DARK SOOTNIN GREASY WASH WATER ALL YEAR'],\n",
       "      dtype='<U52')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/course_project/opensource/deepspeech.pytorch-3.0/deepspeech_pytorch/model.py:303: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) // m.stride[1] + 1)\n"
     ]
    }
   ],
   "source": [
    "org = speech_recognizer.predict(sound, batch_size=1, transcription_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HE PLAYED BASK OF ALL THERE WHILE WORKING TOWARD A LAW DEGREE'],\n",
       "      dtype='<U61')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "[0.05448868 0.24420134 0.40261995 0.24420134 0.05448868]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import gaussian\n",
    "size = 5   # 滤波器大小\n",
    "sigma = 1  # 标准差\n",
    "filter_1d = gaussian(size, sigma)\n",
    "filter_1d /= np.sum(filter_1d)\n",
    "print(filter_1d.shape)\n",
    "print(filter_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve\n",
    "audio_data = np.array([1, 1, 1, 1, 1])\n",
    "filtered_audio = convolve(audio_data, filter_1d, mode=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[0.70130997 0.94551132 1.         0.94551132 0.70130997]\n"
     ]
    }
   ],
   "source": [
    "print(audio_data)\n",
    "print(filtered_audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Mar  2 2023, 03:21:46) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bf74c9b013b24a0d07f5a4ae014fb6841225b14bffc95c8a6be0c177756d31a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
